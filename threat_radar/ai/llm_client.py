"""LLM client abstraction for AI integration"""

import os
import re
import json
from abc import ABC, abstractmethod
from typing import Optional, Dict, Any
from tenacity import retry, stop_after_attempt, wait_exponential
import requests


def repair_json(content: str) -> str:
    """
    Repair common JSON syntax errors produced by LLMs.

    Fixes:
    - Trailing commas in arrays: ["a", "b",] -> ["a", "b"]
    - Trailing commas in objects: {"a": 1,} -> {"a": 1}
    - Multiple trailing commas: [1, 2,,] -> [1, 2]
    """
    # Remove trailing commas before closing brackets/braces
    # Pattern: comma followed by optional whitespace and closing bracket/brace
    content = re.sub(r',(\s*[}\]])', r'\1', content)

    # Remove multiple consecutive commas
    content = re.sub(r',(\s*,)+', ',', content)

    return content


class LLMClient(ABC):
    """Abstract base class for LLM clients"""

    @abstractmethod
    def generate(self, prompt: str, temperature: float = 0.7, max_tokens: int = 2000) -> str:
        """
        Generate a response from the LLM.

        Args:
            prompt: The input prompt
            temperature: Sampling temperature (0.0 to 1.0)
            max_tokens: Maximum tokens to generate

        Returns:
            The generated text response
        """
        pass

    @abstractmethod
    def generate_json(self, prompt: str, temperature: float = 0.7) -> Dict[str, Any]:
        """
        Generate a JSON response from the LLM.

        Args:
            prompt: The input prompt
            temperature: Sampling temperature (0.0 to 1.0)

        Returns:
            Parsed JSON response as dictionary
        """
        pass


class OpenAIClient(LLMClient):
    """OpenAI GPT client implementation"""

    def __init__(self, api_key: Optional[str] = None, model: str = "gpt-4o"):
        """
        Initialize OpenAI client.

        Args:
            api_key: OpenAI API key (defaults to OPENAI_API_KEY env var)
            model: Model name (gpt-4o, gpt-4-turbo, gpt-3.5-turbo, etc.)
        """
        try:
            from openai import OpenAI
        except ImportError:
            raise ImportError(
                "OpenAI package not installed. Install with: pip install openai"
            )

        self.api_key = api_key or os.getenv("OPENAI_API_KEY")
        if not self.api_key:
            raise ValueError(
                "OpenAI API key not provided. Set OPENAI_API_KEY environment variable "
                "or pass api_key parameter."
            )

        self.model = model
        self.client = OpenAI(api_key=self.api_key)

    @retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=2, max=10))
    def generate(self, prompt: str, temperature: float = 0.7, max_tokens: int = 2000) -> str:
        """Generate response using OpenAI API"""
        try:
            # Use max_completion_tokens for newer models (gpt-4 and newer)
            params = {
                "model": self.model,
                "messages": [{"role": "user", "content": prompt}],
                "max_completion_tokens": max_tokens,
            }

            # Some models (like gpt-5-nano) only support temperature=1 (default)
            # Don't add temperature parameter for these models
            if "nano" not in self.model.lower():
                params["temperature"] = temperature

            response = self.client.chat.completions.create(**params)
            return response.choices[0].message.content.strip()
        except Exception as e:
            raise RuntimeError(f"OpenAI API error: {str(e)}")

    @retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=2, max=10))
    def generate_json(self, prompt: str, temperature: float = 0.7) -> Dict[str, Any]:
        """Generate JSON response using OpenAI API"""
        original_content = ""  # Save for error messages
        try:
            params = {
                "model": self.model,
                "messages": [{"role": "user", "content": prompt}],
                "response_format": {"type": "json_object"},
            }

            # Some models (like gpt-5-nano) only support temperature=1 (default)
            # Don't add temperature parameter for these models
            if "nano" not in self.model.lower():
                params["temperature"] = temperature

            response = self.client.chat.completions.create(**params)
            content = response.choices[0].message.content

            # Handle None response
            if content is None:
                raise RuntimeError("OpenAI returned None content")

            content = content.strip()
            original_content = content  # Save original for error messages

            # Handle empty responses
            if not content:
                raise RuntimeError("Empty response from OpenAI")

            # Apply same robust extraction strategies as other providers

            # Strategy 1: Extract from markdown code blocks
            if "```" in content:
                code_blocks = re.findall(r'```(?:json)?\s*\n?(.*?)\n?```', content, re.DOTALL)
                if code_blocks:
                    content = code_blocks[0].strip()

            # Strategy 2: Find JSON object in the text
            json_match = re.search(r'\{.*\}', content, re.DOTALL)
            if json_match:
                content = json_match.group(0)

            # Strategy 3: If content doesn't start with {, find first {
            if not content.startswith('{'):
                start_idx = content.find('{')
                if start_idx != -1:
                    content = content[start_idx:]

            # Strategy 4: Balance braces if extra data after }
            if content.count('}') > content.count('{'):
                brace_count = 0
                for i, char in enumerate(content):
                    if char == '{':
                        brace_count += 1
                    elif char == '}':
                        brace_count -= 1
                        if brace_count == 0:
                            content = content[:i+1]
                            break

            # Strategy 5: Repair common JSON syntax errors
            content = repair_json(content)

            # Final validation before parsing
            if not content or not content.strip():
                raise RuntimeError(f"No valid JSON found in response. Original: {original_content[:200]}")

            if not content.startswith('{'):
                raise RuntimeError(f"Response doesn't contain JSON object. Content: {content[:200]}")

            return json.loads(content)
        except json.JSONDecodeError as e:
            preview = original_content[:300] if len(original_content) > 300 else original_content
            raise RuntimeError(f"Invalid JSON from OpenAI: {str(e)}\nOriginal response: {preview}")
        except Exception as e:
            # Import BadRequestError for specific error handling
            try:
                from openai import BadRequestError
            except ImportError:
                BadRequestError = type(None)  # Fallback if import fails

            # Catch and enhance BadRequestError with better message
            if isinstance(e, BadRequestError):
                error_msg = str(e)
                # Add helpful context for common JSON mode errors
                if "does not support" in error_msg.lower() or "json" in error_msg.lower():
                    raise RuntimeError(
                        f"OpenAI API error: {error_msg}\n\n"
                        f"ðŸ’¡ The model '{self.model}' may not support JSON mode.\n"
                        f"   Recommended models: 'gpt-4o', 'gpt-4-turbo', or 'gpt-3.5-turbo-1106'"
                    )
                else:
                    raise RuntimeError(f"OpenAI API error: {error_msg}")

            if "OpenAI" in str(e) or "response" in str(e).lower():
                raise  # Re-raise our custom errors
            raise RuntimeError(f"OpenAI API error: {str(e)}")


class AnthropicClient(LLMClient):
    """Anthropic Claude client implementation"""

    def __init__(self, api_key: Optional[str] = None, model: str = "claude-3-5-sonnet-20241022"):
        """
        Initialize Anthropic client.

        Args:
            api_key: Anthropic API key (defaults to ANTHROPIC_API_KEY env var)
            model: Model name (claude-3-5-sonnet-20241022, claude-3-opus-20240229, etc.)
        """
        try:
            from anthropic import Anthropic
        except ImportError:
            raise ImportError(
                "Anthropic package not installed. Install with: pip install anthropic"
            )

        self.api_key = api_key or os.getenv("ANTHROPIC_API_KEY")
        if not self.api_key:
            raise ValueError(
                "Anthropic API key not provided. Set ANTHROPIC_API_KEY environment variable "
                "or pass api_key parameter."
            )

        self.model = model
        self.client = Anthropic(api_key=self.api_key)

    @retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=2, max=10))
    def generate(self, prompt: str, temperature: float = 0.7, max_tokens: int = 2000) -> str:
        """Generate response using Anthropic API"""
        try:
            response = self.client.messages.create(
                model=self.model,
                max_tokens=max_tokens,
                temperature=temperature,
                messages=[{"role": "user", "content": prompt}],
            )
            return response.content[0].text.strip()
        except Exception as e:
            raise RuntimeError(f"Anthropic API error: {str(e)}")

    @retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=2, max=10))
    def generate_json(self, prompt: str, temperature: float = 0.7) -> Dict[str, Any]:
        """Generate JSON response using Anthropic API"""
        json_prompt = f"{prompt}\n\nRespond with valid JSON only, no other text."
        original_content = ""

        try:
            response = self.client.messages.create(
                model=self.model,
                max_tokens=2000,
                temperature=temperature,
                messages=[{"role": "user", "content": json_prompt}],
            )
            content = response.content[0].text

            # Handle None response
            if content is None:
                raise RuntimeError("Claude returned None content")

            content = content.strip()
            original_content = content

            # Handle empty responses
            if not content:
                raise RuntimeError("Empty response from Claude")

            # Try multiple extraction strategies

            # Strategy 1: Extract from markdown code blocks
            if "```" in content:
                code_blocks = re.findall(r'```(?:json)?\s*\n?(.*?)\n?```', content, re.DOTALL)
                if code_blocks:
                    content = code_blocks[0].strip()

            # Strategy 2: Find JSON object in the text
            json_match = re.search(r'\{.*\}', content, re.DOTALL)
            if json_match:
                content = json_match.group(0)

            # Strategy 3: If content doesn't start with {, find first {
            if not content.startswith('{'):
                start_idx = content.find('{')
                if start_idx != -1:
                    content = content[start_idx:]

            # Strategy 4: Balance braces if extra data after }
            if content.count('}') > content.count('{'):
                brace_count = 0
                for i, char in enumerate(content):
                    if char == '{':
                        brace_count += 1
                    elif char == '}':
                        brace_count -= 1
                        if brace_count == 0:
                            content = content[:i+1]
                            break

            # Strategy 5: Repair common JSON syntax errors
            content = repair_json(content)

            # Final validation before parsing
            if not content or not content.strip():
                raise RuntimeError(f"No valid JSON found in response. Original: {original_content[:200]}")

            if not content.startswith('{'):
                raise RuntimeError(f"Response doesn't contain JSON object. Content: {content[:200]}")

            return json.loads(content)
        except json.JSONDecodeError as e:
            preview = original_content[:300] if len(original_content) > 300 else original_content
            raise RuntimeError(f"Invalid JSON from Claude: {str(e)}\nOriginal response: {preview}")
        except Exception as e:
            if "Claude" in str(e) or "response" in str(e).lower():
                raise
            raise RuntimeError(f"Anthropic API error: {str(e)}")


class OllamaClient(LLMClient):
    """Ollama local model client implementation"""

    def __init__(self, base_url: str = "http://localhost:11434", model: str = "llama2"):
        """
        Initialize Ollama client.

        Args:
            base_url: Ollama API endpoint
            model: Model name (llama2, mistral, codellama, etc.)
        """
        self.base_url = base_url.rstrip("/")
        self.model = model

    @retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=2, max=10))
    def generate(self, prompt: str, temperature: float = 0.7, max_tokens: int = 2000) -> str:
        """Generate response using Ollama API"""
        try:
            response = requests.post(
                f"{self.base_url}/api/generate",
                json={
                    "model": self.model,
                    "prompt": prompt,
                    "stream": False,
                    "options": {
                        "temperature": temperature,
                        "num_predict": max_tokens,
                    },
                },
                timeout=120,
            )
            response.raise_for_status()
            return response.json()["response"].strip()
        except requests.exceptions.RequestException as e:
            raise RuntimeError(f"Ollama API error: {str(e)}")

    @retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=2, max=10))
    def generate_json(self, prompt: str, temperature: float = 0.7) -> Dict[str, Any]:
        """Generate JSON response using Ollama API"""
        json_prompt = f"{prompt}\n\nRespond with valid JSON only."
        original_content = ""

        try:
            response = requests.post(
                f"{self.base_url}/api/generate",
                json={
                    "model": self.model,
                    "prompt": json_prompt,
                    "stream": False,
                    "options": {"temperature": temperature},
                    "format": "json",
                },
                timeout=120,
            )
            response.raise_for_status()
            content = response.json()["response"]

            # Handle None response
            if content is None:
                raise RuntimeError("Ollama returned None content")

            content = content.strip()
            original_content = content

            # Handle empty responses
            if not content:
                raise RuntimeError("Empty response from Ollama")

            # Try multiple extraction strategies

            # Strategy 1: Extract from markdown code blocks
            if "```" in content:
                code_blocks = re.findall(r'```(?:json)?\s*\n?(.*?)\n?```', content, re.DOTALL)
                if code_blocks:
                    content = code_blocks[0].strip()

            # Strategy 2: Find JSON object in the text
            json_match = re.search(r'\{.*\}', content, re.DOTALL)
            if json_match:
                content = json_match.group(0)

            # Strategy 3: If content doesn't start with {, find first {
            if not content.startswith('{'):
                start_idx = content.find('{')
                if start_idx != -1:
                    content = content[start_idx:]

            # Strategy 4: Balance braces if extra data after }
            if content.count('}') > content.count('{'):
                brace_count = 0
                for i, char in enumerate(content):
                    if char == '{':
                        brace_count += 1
                    elif char == '}':
                        brace_count -= 1
                        if brace_count == 0:
                            content = content[:i+1]
                            break

            # Strategy 5: Repair common JSON syntax errors
            content = repair_json(content)

            # Final validation before parsing
            if not content or not content.strip():
                raise RuntimeError(f"No valid JSON found in response. Original: {original_content[:200]}")

            if not content.startswith('{'):
                raise RuntimeError(f"Response doesn't contain JSON object. Content: {content[:200]}")

            return json.loads(content)
        except requests.exceptions.RequestException as e:
            raise RuntimeError(f"Ollama API error: {str(e)}")
        except json.JSONDecodeError as e:
            preview = original_content[:300] if len(original_content) > 300 else original_content
            raise RuntimeError(f"Invalid JSON from Ollama: {str(e)}\nOriginal response: {preview}")
        except Exception as e:
            if "Ollama" in str(e) or "response" in str(e).lower():
                raise
            raise RuntimeError(f"Ollama error: {str(e)}")


def get_llm_client(
    provider: Optional[str] = None,
    model: Optional[str] = None,
    api_key: Optional[str] = None,
    endpoint: Optional[str] = None,
) -> LLMClient:
    """
    Factory function to get an LLM client based on configuration.

    Args:
        provider: AI provider ('openai', 'anthropic', or 'ollama'). Defaults to AI_PROVIDER env var.
        model: Model name. Defaults to AI_MODEL env var.
        api_key: API key for cloud providers. Defaults to OPENAI_API_KEY or ANTHROPIC_API_KEY env var.
        endpoint: Endpoint URL for local models. Defaults to LOCAL_MODEL_ENDPOINT env var.

    Returns:
        LLMClient instance

    Raises:
        ValueError: If provider is invalid or configuration is missing
    """
    provider = provider or os.getenv("AI_PROVIDER", "openai")
    model = model or os.getenv("AI_MODEL")

    if provider == "openai":
        default_model = model or "gpt-4o"  # Use gpt-4o by default (supports JSON mode)
        return OpenAIClient(api_key=api_key, model=default_model)
    elif provider == "anthropic":
        default_model = model or "claude-3-5-sonnet-20241022"
        return AnthropicClient(api_key=api_key, model=default_model)
    elif provider == "ollama":
        default_model = model or "llama2"
        default_endpoint = endpoint or os.getenv("LOCAL_MODEL_ENDPOINT", "http://localhost:11434")
        return OllamaClient(base_url=default_endpoint, model=default_model)
    else:
        raise ValueError(
            f"Invalid AI provider: {provider}. Supported providers: 'openai', 'anthropic', 'ollama'"
        )
